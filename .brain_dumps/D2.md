There are some interesting points in the OpenAI research that we are able to tokenize. We can adapt the tokenizer for HTML input, so that div, table, etc. become single tokens, not symbols themselves. Maybe Claude is doing this - it's interesting that's why they're better in coding. Okay. 

----

precision, recall, and F1-score for scorring

----

augmenting training

---

Okay, what do we have right now? What plan do we have right now? - What we need to do: We need to prepare the dataset. For the dataset, I think we will use some real-world data. We will take some I already have a script which marks all the content with the xdata tag. We will take this script, run it on raw HTML, and then save it into files separately which later we will use for extraction. What about labeling? I think we will label at least 10 websites. We will label all the links in the HTML and take some complex ones (e.g., GitHub which contains a lot of shitty links). We will check this information. Next up will be some quietly interesting sites (like Google Search page) and we'll decide more. Next, we will run this data through the system to label it manually. Then we will run this data through the cloud to create more data (e.g., to create more variations for specific buttons or something like this). Finally, we will use this data to train models (e.g., Mistral and LLaM) and see what the result is.

---

Now we need to define the two main purposes: - Practical purpose: Practical purpose: We need to have a method which will run a headless browser - Go to the page - Activate the page Head full browser activates a page. We click the button; it takes all the data and all the HTML already marked and feeds it to the AI and saves it into the file. - After that we need to have static analyzer which analyzes and then runs through the already fine-tuned model

---

Also, we need to test this approach to see if it's better than fitting the same inputs to the general-purpose models. What are the benefits? We will take Claude, maybe Haiku, yeah I think Haiku or maybe 3 and 7 in batch mode and GPT, and we will check the results. What are the results? What's the amount of errors? For example, we will run with retry 3. Let's imagine we will implement basic markup fixes with Mistral. If there are some issues with it with the markup, that's all. 

The final metric is the amount of errors. Quality of the extraction. We will check the vector similarity of the labeled data and the data labeled by AI. If it is close, it is okay. If it is not, it is not okay and we will rate it from 0 to 1. I think. 
