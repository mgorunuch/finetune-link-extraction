Fine-Tuning LLMs for HTML Data Extraction
Introduction
Extracting structured data (e.g. JSON) from raw HTML is a long-standing challenge in web scraping. Traditional methods require hand-crafted parsers and brittle rules to handle varied websites. Large Language Models (LLMs) offer a new paradigm: they can learn to interpret HTML and output structured information by understanding context and semantics, rather than relying on explicit CSS selectors or regex. Recent advances show that LLMs like Claude (Anthropic), Llama (Meta), and Mistral (open-source) can be leveraged for general-purpose HTML data extraction. This report examines how to fine-tune these models for the task, covering dataset creation, zero-shot vs. fine-tuned performance, techniques for complex layouts, and empirical results. We focus on high-level frameworks and findings from both academia and industry, rather than low-level implementation details.
Challenges in Parsing HTML with LLMs
Parsing HTML into structured data is non-trivial due to several challenges:
Complex, Nested Structure: Web pages are hierarchical (divs, lists, tables within tables, etc.). An extractor must preserve this nested structure in the output (e.g. nested JSON). LLMs must therefore understand HTML markup and hierarchy, not just flat text. This is difficult if the model isn’t trained to pay attention to tags and nesting.
Diverse Layouts: There is no single schema for all webpages – each site or page type can have a unique structure and field set. A general model must adapt to highly variable site-specific HTML structures
reddit.com
. In practice, a schema that works for one page (say a product page with price, description, reviews) might be completely different on another page. This variability means a one-size-fits-all prompt can fail when the content shifts.
Inconsistent Formatting: Even for the same type of data, different sites or authors use different formats. Dates, addresses, prices, etc., might appear in many forms (e.g. “Jan 5, 2025” vs “2025-01-05” for dates). Some pages may have missing fields or additional irrelevant text. LLMs must learn to identify the relevant content despite extraneous HTML markup or stylistic differences.
Dynamic Content and Scripts: Modern websites often load content dynamically via JavaScript or personalize the HTML per user/session. Traditional scrapers struggle with such pages
promptcloud.com
. For LLMs, the challenge is that the raw HTML might not contain the final rendered content. A best practice is to pre-render or use a headless browser to capture the full HTML DOM (after scripts execution) before feeding it to the model. Even then, dynamic sections like expanding menus or interactive widgets produce HTML that’s not straightforward. Fine-tuning can help a model learn to ignore irrelevant script/style content and focus on meaningful text.
Long Context Length: Some pages are very long (multi-thousand tokens of HTML). Models like Claude have a large context window (up to 100k tokens in Claude 2) which can handle very long HTML in one go, but many open models (Llama, Mistral) have shorter contexts (4k to 8k tokens). Handling lengthy pages may require chunking the HTML into parts and processing sequentially
researchgate.net
. If the model isn’t fine-tuned on such chunked inputs, it might lose coherence across splits.
These challenges mean that out-of-the-box LLMs might misidentify fields, omit data, or produce malformed output when asked to parse arbitrary HTML. Fine-tuning is proposed to mitigate these issues by teaching the model with examples covering diverse structures and the exact output format required.
Building Training Datasets from HTML to Structured Data
A crucial step in fine-tuning is creating a high-quality training dataset of HTML pages paired with the desired structured output. Several methodologies can be used, often in combination:
Manual Annotation: Curate a representative set of webpages from various domains (news articles, product pages, profiles, etc.) and have human annotators label the key information in each (or directly create the JSON output). This yields high-quality data, but it’s labor-intensive and slow for large scales. You might use annotation tools or frameworks to highlight elements in rendered pages and assign them to a JSON schema. Because HTML data extraction has no one universal schema, one strategy is to define a generic JSON format that can represent common structures (sections, lists, key-value pairs). For example, a JSON schema might have a list of sections, each with a title and content, or use keys that mirror HTML tags (like a "ul" key containing an array of list items). Annotators would map the HTML content into this generalized JSON structure. The fine-tuned model can then learn to produce a structured representation for any page in that format.
Distant Supervision from Existing Structured Data: Many websites embed structured metadata (like JSON-LD scripts, OpenGraph tags, or microdata) that describe the page content (for SEO or social sharing). These can serve as noisy labels for training. For instance, one can crawl e-commerce pages and use the product info from the JSON-LD metadata as the target output, with the raw HTML as input. Similarly, news sites often embed article metadata (author, date, etc.). By using these as ground truth, large training corpora can be assembled without manual labeling. However, the model must learn to extract that same info from the HTML text, which may be challenging if the metadata doesn’t align perfectly with visible content.
Synthetic Data Generation: To cover a broad spectrum of page structures, recent research suggests generating synthetic training examples using LLMs themselves
medium.com
medium.com
. This approach was demonstrated by Huang (2024) for document extraction: with a small seed set of real examples, they prompted a strong LLM to create additional pseudo-data
medium.com
. For HTML, one could prompt a model like GPT-4 or Claude to “invent” an HTML snippet containing certain data and simultaneously output the structured JSON for it. For example, ask the model to generate a fictional product page HTML (with <div>s, <ul>s, etc.) about a sample product, and also generate the JSON with fields like name, price, specs, reviews. The synthetic HTML+JSON pairs can then augment the training set. The key is to ensure the synthetic HTML is realistic (e.g., containing typical noise and varied layouts) and that the JSON correctly reflects it. Synthetic data helps address the data scarcity problem – fine-tuning works best with thousands of examples, whereas one might have only dozens of real labeled pages. Using LLMs to hallucinate HTML pages and labels is effective because LLMs have seen many web pages during pretraining and can mimic their structure. As an example, Databricks engineers used an LLM (Meta’s Llama-based model) to generate additional training documents for extraction; starting from 50 real samples, they produced two rounds of ~40 synthetic samples, which significantly boosted accuracy
medium.com
.
Ensuring Diversity: However the data is obtained, it should cover as many variations as possible – different domains (news, products, forums, etc.), different languages or encodings, pages with tables vs. lists, deeply nested vs. shallow content, etc. Diversity teaches the model to generalize. One should also include some difficult cases (edge cases) in the training mix: pages with missing fields, unusual formatting, or extra noisy HTML. This can be done by deliberately selecting a few “messy” pages and annotating them. Without such cases, a fine-tuned model might perform well on typical pages but fail on any page that deviates from the norm.
Input-Output Format in Training: In fine-tuning an LLM for extraction, we usually frame it as a supervised learning task where the input is the HTML (often prefaced with an instruction) and the output is the structured data. A common practice is to use an instruction-tuning format: e.g., the training sample might be: Prompt: "Convert the following HTML content into a JSON with the required fields: <HTML> ... </HTML>", Response: "{ ...JSON output... }". In fine-tuning frameworks like Alpaca or LoRA, this would be stored as a single sequence with special tokens for prompt and completion. Ensuring that the JSON output is always valid and follows the schema in the training examples is critical – the model will learn what it sees. Some practitioners validate the outputs with a parser during dataset creation (e.g. using Pydantic or JSON schema) to filter out any malformed examples
mlops.systems
mlops.systems
. This way, the fine-tuned model is less likely to produce invalid JSON or incorrect keys.
Tokenizer and HTML: HTML contains many symbols (<, >, /, =) and tokens like tag names or attributes that the base tokenizer might split oddly. One subtle optimization is to adapt the tokenizer for HTML/JSON. For instance, ensuring that common tag names (like <div>, <table>, </ etc.) are treated as distinct tokens can help the model learn structure more easily. Some fine-tuners include the vocabulary of HTML tags or JSON tokens in the tokenizer so that the model can more directly attend to them. (In one tutorial, developers inserted XML/HTML tag tokens into the tokenizer to better handle markup structure
youtube.com
.) This isn’t strictly necessary, but if one notices the model frequently gets confused by tags, a tokenizer tweak plus fine-tuning on those tokens can improve parsing accuracy.
In summary, creating a training set for HTML extraction often involves a mix of modest real examples and a large number of synthetic or auto-labeled examples, all formatted consistently. Fine-tuning on such data allows the LLM to specialize in “reading” HTML as input and producing structured outputs, much like how a human web scraper developer would operate but now learned end-to-end by the model.
Zero-Shot vs. Fine-Tuned Model Performance
One key question is: can large language models do HTML extraction without fine-tuning (zero-shot), and what gains do we get by fine-tuning? The answer depends on the model size and the complexity of the task:
Zero-Shot with Powerful Models: The largest, most advanced models (GPT-4, Claude 2, etc.) have demonstrated surprisingly strong abilities to parse structured content with just prompting. For example, Anthropic’s Claude has been touted to “understand the webpage’s context and extract data accurately,” even handling complicated nested structures and dynamic content better than traditional scrapers
medium.com
. In a 2024 experiment by Ramadhan, Claude (v3.5) was pitted against GPT-4 and Google’s PaLM (Gemini) on a web scraping task (extracting book information from a sample site). Claude achieved a perfect extraction – 0 mistakes out of 30 items – with a simple prompt, making it the most accurate model in that test
serpapi.com
. GPT-4 was nearly as accurate (and balanced in speed), while the PaLM-based model was fastest but slightly less precise
serpapi.com
. This shows that with the right prompt, top-tier models can parse a fairly structured page and output flawless JSON in some cases. Another advantage of models like Claude is context length: it can ingest very long HTML (tens of thousands of tokens) in one go, reducing the need for splitting the input.
Zero-Shot Limitations: Despite these successes, zero-shot prompting has limits. Smaller open-source models (7B–13B parameters) are much less consistent. Users report that a model like Mistral-7B (a highly capable 7B open model) often fails to reliably produce JSON when prompted for extraction – for instance, it might regurgitate the instruction or output only part of the data
reddit.com
. This inconsistency arises because, out-of-the-box, such models haven’t been specifically tuned to follow instructions that require structured output. Even larger instruct-tuned models (e.g. Llama-2 70B Chat) may attempt to output JSON but can make formatting errors or omit subtleties if the prompt isn’t carefully crafted. Moreover, zero-shot prompts might not generalize to all page types: a prompt that works well on one page could confuse the model on a page with a different layout, since the model has no “memory” of how to handle the new structure beyond its pretrained knowledge. In one baseline evaluation using GPT-4 (GPT-3.5 Turbo) on a structured text extraction task, the model achieved about 71% F1-score in identifying the correct fields
medium.com
. This was decent, but left nearly 30% of the information missing or incorrect – “not a score you’d be comfortable with putting into production” as the author noted
medium.com
. Clearly, there was room for improvement either via better prompting (few-shot, tools) or via fine-tuning. Some have found that providing few-shot examples or using specialized prompting features (like OpenAI’s function calling or Anthropic’s JSON mode) can boost zero-shot performance. For example, enumerating the expected JSON schema in the prompt can greatly improve output consistency – one practitioner saw correct JSON outputs jump from ~30% to nearly 100% just by explicitly describing every expected field in the prompt
reddit.com
. This kind of prompt engineering (essentially on-the-fly schema tutoring) helps guide the model, but it requires knowing the fields in advance. If the page structure is unknown a priori, a single prompt can’t enumerate it.
Fine-Tuning Benefits: Fine-tuning an LLM on HTML-to-data examples fundamentally changes its behavior on extraction tasks. The model no longer relies solely on its general knowledge (or clever prompts) to figure out what to do – it has seen similar tasks during training and thus is more prepared. Empirically, fine-tuning yields several benefits:
Higher Accuracy: Fine-tuned models capture the patterns of extraction, leading to higher precision and recall. For instance, after fine-tuning a smaller 8B model on a set of documents, its F1-score matched the zero-shot performance of a 70B model (both around 70–71% F1)
medium.com
. In other words, specialization allowed an 8B model to perform on par with a model nearly 9× larger, which is a huge win for efficiency. Moreover, by augmenting the fine-tuning with more data, the fine-tuned model began to surpass the base-model baseline. Huang (2024) reports that adding synthetic training examples improved F1 by ~5% absolute in one round
medium.com
, and nearly another 5% with a second round of data
medium.com
, ultimately pushing the F1 close to 80% on that task. Each fine-tuning iteration made the model more accurate at extracting the right fields from text. Similarly, anecdotal evidence from community forums suggests that fine-tuned Llama-2 models can achieve very reliable JSON output for specific domains (e.g. parsing academic references or forum posts) where zero-shot often failed. The fine-tuned model is less likely to hallucinate or skip content because it has learned a deterministic style of responding: it treats the task as a transformation problem, not an open-ended generation.
Consistency and Format Adherence: A fine-tuned extraction model is typically trained to output only the JSON (or XML, etc.) with no extra commentary, every single time. Zero-shot models, even when prompted to “just output JSON,” can sometimes include conversational fluff or apologize for errors, especially if the prompt isn’t perfect. Fine-tuning on hundreds of strict input-output examples conditions the model to the required format. For example, Anthropic’s documentation notes that precisely defining the output format in the prompt is important for Claude to obey JSON formatting
docs.anthropic.com
. Fine-tuning effectively bakes this formatting compliance into the model’s weights. As a result, you’ll see far fewer cases of broken JSON or missing brackets. In fact, one of the goals in fine-tuning is to eliminate post-processing – you want the model’s raw output to be a directly parseable JSON 99+% of the time.
Robustness to Variation: When an LLM is fine-tuned on diverse HTML examples, it learns to extract in a more generalizable way. Instead of being overly tied to one prompt wording or one page style, it develops an internal “skill” of translating web content to structured data. This means it can handle sites it never saw in training, as long as they’re somewhat similar to patterns learned. For example, a fine-tuned model might learn that <ul><li>...</li><li>...</li></ul> usually indicates a list of items and thus output a JSON array for that content, regardless of the actual text of the items. Zero-shot models might not consistently make that connection. In essence, fine-tuning imparts a bias toward extracting information and ignoring irrelevant text. Researchers note that base LLMs often prioritize fluent text generation over factual extraction
researchgate.net
. Fine-tuning flips that priority – the model becomes more “clinical” in plucking out facts from HTML and less likely to generate unnecessary text.
Cost and Feasibility: It’s worth noting that fine-tuning has costs: one must gather data and run training (which, for large models, can be resource-intensive). For closed models like Claude or GPT-4, fine-tuning isn’t even available to end-users as of 2025 (OpenAI only allows fine-tuning on some smaller models, and Anthropic has not opened Claude for third-party fine-tuning). Therefore, practitioners often use a hybrid approach: leverage the power of closed models zero-shot for quick prototyping, but invest in fine-tuning open models (Llama 2, Mistral, etc.) for deploying scalable solutions that can run locally. The good news is that techniques like LoRA (Low-Rank Adaptation) allow fine-tuning of even 7B–13B models on a single GPU, making it accessible to many developers. Fine-tuning a model like Llama-2 13B or Mistral 7B on a few thousand HTML examples can be done in hours, and the resulting model can run inference on consumer hardware. The fine-tuned model, once created, will also have lower latency than hitting a large API model for each request.
In summary, zero-shot LLMs (especially large ones) can impress with their ability to do HTML extraction with clever prompting – even achieving perfect results in controlled cases
serpapi.com
. But for robust, repeatable extraction across the wild variety of the web, fine-tuning an LLM on structured extraction tasks yields more reliable and higher-quality results
medium.com
medium.com
. Fine-tuned smaller models can rival much larger models’ performance, and they stick to the expected output format more strictly.
Techniques for Improving Extraction Accuracy
Fine-tuning alone does not solve all problems. Here we outline additional techniques and best practices – applicable during prompting, fine-tuning, or inference – to boost accuracy, especially for complex pages and varying layouts:
Schema Definition and Controlled Output: Clearly defining the target schema (what fields to extract, in what JSON structure) greatly aids both training and inference. If you can predetermine a schema for the content (even a flexible one), use it. In prompts or training instructions, list the keys expected. The model then has a framework to fill and is less likely to wander. In cases where the schema itself varies by page, one novel approach is a two-pass extraction
reddit.com
: use the LLM first to analyze the page and propose a JSON schema or outline (essentially, identify what information is present), and then in a second pass have the LLM populate that schema with the page’s data. This two-agent (or two-step) method was suggested to handle pages with low structural commonality
reddit.com
. For example, Agent1 reads an academic webpage and says: “This page looks like a profile with fields: Name, Institution, Publications list.” Then Agent2 is prompted with “Extract Name, Institution, Publications from the page HTML” to produce the JSON. This approach dynamically adapts to each page’s structure. While more complex, it can be very effective when doing general web extraction where you truly don’t know what the content will be beforehand.
Chain-of-Thought for Extraction: Normally we want just the JSON output, but giving the model the ability to reason internally can improve correctness. One way is to allow the model to generate a brief intermediate reasoning (hidden from the final output) where it, for instance, lists the key pieces of info it found and then converts that to JSON. In OpenAI models this can be done with the function_call system or a structured prompt; for open models, one can fine-tune them to produce a special delimiter (like <reason> ... </reason>) with reasoning that is later stripped out. Empirically, this can reduce omissions – the model, by reflecting on the content, is less likely to miss a field. However, including chain-of-thought in fine-tuning data requires careful crafting of training examples (so that the model learns to produce the reasoning). Many fine-tuning efforts skip this and rely on direct mapping, but it’s a potential area of improvement as noted in IE research.
Focus on Hierarchy: To handle nested HTML structures, train or prompt the model to preserve the hierarchy in output. For example, if a page has sections and subsections, the JSON should reflect that (e.g., an array of sections, each with its own sub-items). During fine-tuning, one can include representations of the HTML tree. A simple trick is to include indentation or markers in the HTML input to denote nesting depth (for instance, replacing <div class="subsection"> with \n>> Subsection:\n in the input string). The model then picks up on these cues to structure the JSON. Additionally, ensure the training set contains deeply nested examples – e.g., a page with a list inside a table inside another list – so the model learns to output nested lists/objects accordingly. If the model still struggles with nesting, an alternative is to break the task: first extract top-level sections, then for each section feed its HTML snippet to get sub-sections, and so on. This hierarchical processing can be orchestrated with a small script or an agent loop.
Handling Dynamic and Noisy Content: As mentioned, dynamic content requires feeding the model the fully rendered HTML or at least the text that a user would see. That means using a tool (like Puppeteer, Selenium, or a headless browser API) to get the HTML after scripts. During fine-tuning, if your task domain includes such pages, incorporate examples of post-JS HTML. Similarly, remove irrelevant parts of the HTML that you never want in the output. For example, navigation menus, footers, and advertisements are usually not desired. You can pre-clean the HTML by stripping <nav>...</nav> sections or any repeated template sections. Another approach is to instruct the model (via prompt or learned behavior) to ignore certain sections. One can add hints in the HTML input like <!-- ignore: footer start --> ... <!-- ignore: footer end --> as markers around irrelevant content; a fine-tuned model can learn to skip anything in such comments. Consistently applying this during training will teach the model to do the same on unseen pages.
Chunking Long Pages: When dealing with very long pages that exceed the model’s input length (which is often the case for full HTML of big pages), chunking is necessary. The question is how to chunk without losing context. A simple strategy is to split by logical sections (e.g., split at <div class="section"> boundaries or paragraph boundaries) and include some overlapping text between chunks to avoid cutting important info in half. The extraction can then be done chunk-by-chunk and results merged. Fine-tuning can help the model handle partial context: if the model is trained on chunks with knowledge that more content may follow, it can be taught either to output partial JSON and indicate continuation, or to output complete JSON from a chunk and rely on an external process to merge. An alternative approach is to use a Retrieval-Augmented Generation (RAG) framework
researchgate.net
researchgate.net
: treat the HTML as a knowledge corpus, use a retriever to fetch relevant parts (perhaps via semantic search for keywords like “Price:” or “Specifications”), and feed those to the model to extract specific fields. This can reduce the amount of text the model needs to read at once. Research by Ahluwalia and Wani (2023) found that a RAG approach (combining chunking, searching, and ranking of HTML segments) enabled efficient extraction of complex data with LLMs, rivaling specialized HTML-specific models
researchgate.net
researchgate.net
. In practice, if fine-tuning is done on full pages but at inference you must chunk, you should maintain some coherence: for example, always include the page title or header in each chunk as context so the model knows what the content is about.
Post-Processing and Validation: Even with fine-tuning, it’s wise to include a validation step in your pipeline. One best practice is to run the model’s JSON output through a schema validator or simple Python checks (e.g., ensure required keys exist, values make sense). If the model output fails, you can have a fallback: either prompt the model again with an error message (“The output was invalid, please correct it”) or use a second LLM pass solely to fix JSON syntax. Some teams even fine-tune a secondary model as a “JSON repair assistant.” However, with a well-fine-tuned primary model, such cases should be rare. Still, the system should be robust to the occasional error. Logging model outputs and their errors can also guide further fine-tuning: if you see systematic mistakes (e.g., model often mis-labels a particular field), you can add more training examples focusing on that or adjust the prompt.
Leverage Prompting Tricks in Fine-Tuning: It’s not an either/or between prompting and fine-tuning – you can use both. For example, if few-shot prompting helped in zero-shot mode, you can incorporate a similar approach in the fine-tuning data. Concretely, you could prepend one or two demonstration Q&A pairs (HTML → JSON examples) in the prompt during training, teaching the model to effectively operate in a few-shot setting. Then at inference, you could do the same. However, this costs tokens and fine-tuning can usually eliminate the need for providing examples each time. Another trick is to use system messages or roles (if the model supports it) to steer format. For open models, one can simulate a “system instruction” by a special token at the beginning saying e.g., "<format: JSON only>". Fine-tuning data can include this token, conditioning the model to always follow that rule.
Model-Specific Considerations: Each model may have quirks. Claude tends to be very good at following instructions and has built-in support for JSON formatting if you ask for it (Anthropic even suggests a special syntax or asking Claude to output an object directly)
docs.anthropic.com
. Claude’s strength is handling very large inputs, so one technique is to feed it a whole page and ask for a summary JSON of key info – Claude can internally attend to long-range dependencies (like matching a table header with table cells far apart). For Llama 2 and Mistral, which are smaller and have less context, the fine-tuning should emphasize precision. These models might need more explicit instruction (e.g., a prompt like “Output JSON only, no explanations” might be included in every training example’s prompt) to prevent any extraneous output. Mistral, being only 7B, might struggle with very long or very complex pages even after fine-tune, so it may be best suited to narrower extraction tasks (or one could consider splitting the task among multiple smaller models each fine-tuned to a subtype of page). If using Llama 2 70B, you have a powerful base that, when fine-tuned, can likely approach the quality of Claude or GPT-4 on many tasks, albeit with a shorter context window. Always monitor each model’s output on a validation set – some may have a tendency to drop certain fields, and you might correct that by slightly altering the prompt template or adding more examples of that field.
To summarize these techniques: Treat the extraction like a careful dialogue with the model – guide it with structure (schema, intermediate reasoning), limit its freedom to wander (by strong formatting instructions or learned behavior), and break down the problem (via chunking or multi-step extraction) when the page is too complex for a single shot. Fine-tuning provides the foundation of knowledge, and these additional strategies build on that foundation to handle the toughest cases (highly nested content, dynamic pages, etc.) with improved accuracy.
Empirical Results and Benchmarks
Measuring the success of an HTML extraction model is typically done with metrics like precision, recall, and F1-score on a set of ground-truth annotated pages. Precision measures how many of the model’s outputs are correct, while recall measures how much of the total desired information the model actually extracted. The F1-score is the harmonic mean of the two, offering a single overall accuracy measure. Fine-tuning has shown clear improvements in these metrics across various experiments:
Baseline vs Fine-Tuned Performance: In one case study (Huang 2024), the task was extracting fields from legal contract documents (not HTML but similarly structured text). A baseline zero-shot run with a powerful LLM achieved 71% F1
medium.com
. After fine-tuning a smaller 8B model on 50 real samples (no augmentation), it achieved ~70% F1 – essentially matching the baseline with a much smaller model
medium.com
. This already demonstrated the efficiency gain: a model ~9× smaller performing at the same level
medium.com
. More importantly, by augmenting training with synthetic data, the fine-tuned model’s F1-score climbed: roughly 75% after one round of synthetic fine-tuning (+5%)
medium.com
medium.com
, and ~80% after a second round (+another 5%)
medium.com
. These quantitative gains underscore that fine-tuning improved both precision and recall – the model missed fewer fields and made fewer incorrect extractions as it saw more varied examples. The improvements were especially pronounced for fields that were rare or tricky in the initial training set; synthetic data helped cover those.
Claude vs Others: In the SerpAPI experiment on parsing a book list page, Claude’s performance was a perfect 30/30 correct extractions
serpapi.com
. GPT-4 was very close (it’s implied it had maybe 1 mistake or so, given it was described as “well balanced” but not explicitly stated as perfect). Google’s Gemini (an early version) was fastest but likely had a few errors, making Claude the most precise model in that test
serpapi.com
. This aligns with anecdotal reports that Claude is extremely good at structured output when properly instructed – possibly due to Anthropic’s training focus on instruction-following. However, one must note these were API models not fine-tuned by the user; they are effectively zero-shot. For a fair comparison, if we fine-tuned an open model on similar tasks, we might approach that level of accuracy on that specific distribution of pages.
Open-Source LLM Benchmarks: There isn’t a single standardized benchmark yet for “general web page extraction,” but we can extrapolate from related IE (Information Extraction) benchmarks. A recent survey of generative IE methods notes that LLMs are increasingly competitive with traditional extraction systems across various tasks
arxiv.org
. On tasks like WikiInfo (extracting infobox data from Wikipedia text) or WebSRC (question answering from web pages), models like GPT-4 have achieved high precision/recall in research settings – often 80-90% F1 – whereas smaller models lag behind without fine-tuning. With fine-tuning, models like Llama-2 have been reported to substantially close the gap. For instance, a fine-tuned 13B model might go from, say, 50% F1 zero-shot to 70%+ F1 on a complex extraction dataset (exact numbers vary by task). The takeaway is that fine-tuning consistently yields significant improvements in structured extraction accuracy across the board, often on the order of +10 to +30 percentage points in F1 depending on baseline, as it adapts the model to the domain specifics.
Error Analysis: What kinds of errors are reduced by fine-tuning? Common baseline errors include: missing a field entirely (recall error), misidentifying text (precision error, e.g., capturing an extra word or picking up a template string like “Advertisement” as if it were data), or formatting mistakes (e.g., outputting a number as a string with a dollar sign when it should be numeric). Fine-tuned models show marked improvement in recall – they learn to find all occurrences of items that should be extracted. For example, if a page has multiple product entries, a good model will output every one of them in a list. Zero-shot might overlook some, perhaps because they appear in a slightly different sub-section. Precision gains come from the model learning to be conservative about what to output: it will include only what matches the expected fields and not ad-lib extra info. In evaluations, one often sees fine-tuned models stop including irrelevant text that a zero-shot model might have included out of confusion. In an evaluation of GPT-4 vs a fine-tuned model on press releases, the fine-tuned model made fewer mistakes in field categorization (e.g., correctly distinguishing a “location” mention as a city vs a province because it had learned the patterns from training)
mlops.systems
mlops.systems
.
Human-level Performance: Is a fine-tuned LLM as good as a human annotator or a dedicated parser? For straightforward pages (where the info is clearly demarcated), we are reaching human-level accuracy in many cases. A score of ~95-100% precision/recall is not uncommon in narrow settings (like extracting products from a known e-commerce layout) with either a highly capable model or a specialized fine-tune. For truly diverse “any website” scenarios, humans still have an edge in adapting to completely new formats. The best results often come from a combined approach – LLM-based extraction with a human in the loop for quality checks on critical data. That said, the frontier is moving fast: as models get fine-tuned on increasingly broad web data, their ability to generalize improves.
In conclusion, quantitative benchmarks so far illustrate that fine-tuning large language models can substantially boost the precision and recall of HTML data extraction, turning a mediocre zero-shot performance into something approaching production-worthy accuracy
medium.com
medium.com
. With careful dataset design and modern fine-tuning techniques, even smaller open models (like Llama-2 13B or Mistral 7B) can become proficient “parsers,” reducing errors to a level that rivals much larger models used in zero-shot mode
medium.com
. This enables more cost-effective deployment of extractors and opens the door to automating web data collection tasks that previously needed constant maintenance.
Conclusion
Fine-tuning LLMs for general-purpose HTML extraction is an emerging solution to a classic problem. By training models on rich pairs of HTML and structured outputs, we imbue them with the ability to act as flexible, intelligent web scrapers that understand context and semantics, not just brittle rules. We discussed how to build training datasets (including leveraging synthetic data and existing metadata), and saw that fine-tuned models offer clear advantages in accuracy and reliability over zero-shot prompts – smaller models can match or beat larger ones when specialized
medium.com
, and output formats become far more consistent. Techniques like multi-step extraction, chunking, and schema-guided prompting can further improve handling of complex and varied webpages. From a theoretical standpoint, this approach aligns with the view of IE as a sequence-to-sequence problem: the HTML is an input language, and the structured JSON is an output language. Fine-tuning teaches the model that “language,” while exploiting the powerful generalization abilities of LLMs to handle synonyms, paraphrases, and unseen structures. Empirical results are promising, with fine-tuned LLMs achieving high precision/recall on diverse extraction tasks – often improving F1-scores by tens of percentage points compared to zero-shot attempts
medium.com
. Looking forward, as research like LLM4IE suggests, we will see even closer integration of LLMs with information extraction tasks, possibly combining them with retrieval systems for better scalability
researchgate.net
. For now, best practices include: clearly define your output schema, use robust training data covering many cases, fine-tune with careful validation, and don’t shy away from creative prompting strategies even with a fine-tuned model. By doing so, one can build a state-of-the-art HTML data extraction pipeline powered by LLMs that significantly reduces the need for manual parser development and maintenance. Sources:
Huang, D. et al. (2024). Structured Extraction with LLM – Part 1 & 2. Databricks Blog – baseline vs fine-tuned performance on information extraction
medium.com
medium.com
medium.com
.
Strick van Linschoten, A. (2024). Finetuning LLMs for structured data extraction. Blog post – methodology of dataset creation and fine-tuning results
mlops.systems
mlops.systems
.
Reddit discussion (2025). LLM for structured HTML content extraction – community tips on model choices, prompt engineering, and schema-first approaches
reddit.com
reddit.com
.
Anthropic (2023). Claude API Documentation – guidelines on obtaining consistent JSON outputs from Claude
docs.anthropic.com
.
SerpApi experiment (2024). Web Scraping with AI: Parsing HTML to structured data – comparative evaluation of Claude, GPT-4, etc. (Claude achieved 0 errors out of 30)
serpapi.com
.
Ahluwalia, A. & Wani, S. (2023). Leveraging LLMs for Web Scraping – research preprint proposing chunking and retrieval strategies for LLM-based extraction
researchgate.net
.
PromptCloud (2025). How LLM Web Scraping Transforms Data Extraction – industry perspective on using and fine-tuning LLMs for web data, highlighting dynamic content handling
promptcloud.com
promptcloud.com
.